# 1.3B.3 Migration Strategies & Schema Evolution

## ðŸ“‹ Section Overview
- **Duration**: 45 minutes
- **Prerequisites**: Understanding of ORM principles, Code-First vs Database-First approaches, and basic database administration
- **Learning Objectives**: 
  - Master different migration strategies for schema evolution
  - Understand safe migration practices and rollback strategies
  - Learn to handle complex schema changes without data loss
  - Implement zero-downtime migrations for production systems
  - Design schema versioning and deployment workflows
- **Difficulty Level**: Intermediate to Advanced

---

## ðŸŽ¯ What You'll Learn

By the end of this section, you will:
- âœ… Implement safe migration strategies for schema changes
- âœ… Design zero-downtime deployment workflows
- âœ… Handle complex data transformations during migrations
- âœ… Manage schema versioning across different environments
- âœ… Create rollback strategies for failed migrations
- âœ… Coordinate team-based schema evolution processes

---

## ðŸ“– Content

### Understanding Schema Evolution

Schema evolution is like **renovating a busy airport while flights continue to operate**. You need to carefully plan each change, ensure passenger safety (data integrity), maintain operations (zero downtime), and have contingency plans (rollbacks) ready. Just as airports use phased construction and temporary routes, database schema evolution requires strategic planning and incremental changes.

### ðŸ›« The Living Airport Analogy

```
ðŸ›« Airport Operations = ðŸ—„ï¸ Database Operations

ðŸ—ï¸ Terminal Expansion (Adding New Tables)
â”œâ”€â”€ Build new terminal â†’ Create new tables/columns
â”œâ”€â”€ Connect walkways â†’ Establish relationships
â”œâ”€â”€ Test all systems â†’ Validate constraints and indexes
â”œâ”€â”€ Route passengers â†’ Migrate data gradually
â”œâ”€â”€ Open for operations â†’ Deploy to production
â””â”€â”€ Monitor traffic flow â†’ Track performance and errors

ðŸ”„ Runway Renovation (Modifying Existing Schema)
â”œâ”€â”€ Close one runway at a time â†’ Use blue-green deployments
â”œâ”€â”€ Redirect flights â†’ Implement backward compatibility
â”œâ”€â”€ Upgrade infrastructure â†’ Transform data in phases
â”œâ”€â”€ Test with limited traffic â†’ Staged rollout process
â”œâ”€â”€ Open renovated runway â†’ Complete migration
â””â”€â”€ Decommission old â†’ Clean up deprecated schema

ðŸš¨ Emergency Protocols (Rollback Strategies)
â”œâ”€â”€ Immediate flight diversion â†’ Quick rollback procedures
â”œâ”€â”€ Passenger safety first â†’ Data integrity protection
â”œâ”€â”€ Alternative routes â†’ Fallback systems ready
â”œâ”€â”€ Communication plan â†’ Team coordination protocols
â””â”€â”€ Post-incident review â†’ Migration retrospectives
```

---

## ðŸ—ï¸ Migration Strategy Types

### 1. Forward-Only Migrations (Standard Approach)

Forward-only migrations are the most common approach, where changes are applied sequentially and irreversibly, similar to how a building is constructed floor by floor.

#### **Prisma Forward Migration Example**

```prisma
// Initial schema
model User {
  id    Int     @id @default(autoincrement())
  email String  @unique
  name  String?
  
  @@map("users")
}
```

```bash
# Create initial migration
npx prisma migrate dev --name init
```

**Migration 001: Add User Profile**

```prisma
// Updated schema
model User {
  id      Int      @id @default(autoincrement())
  email   String   @unique
  name    String?
  profile Profile?
  
  @@map("users")
}

model Profile {
  id     Int     @id @default(autoincrement())
  bio    String?
  avatar String?
  userId Int     @unique @map("user_id")
  
  user   User    @relation(fields: [userId], references: [id], onDelete: Cascade)
  
  @@map("profiles")
}
```

```bash
# Generate migration
npx prisma migrate dev --name add_user_profiles
```

**Generated Migration File:**

```sql
-- CreateTable
CREATE TABLE "profiles" (
    "id" SERIAL NOT NULL,
    "bio" TEXT,
    "avatar" TEXT,
    "user_id" INTEGER NOT NULL,

    CONSTRAINT "profiles_pkey" PRIMARY KEY ("id")
);

-- CreateIndex
CREATE UNIQUE INDEX "profiles_user_id_key" ON "profiles"("user_id");

-- AddForeignKey
ALTER TABLE "profiles" ADD CONSTRAINT "profiles_user_id_fkey" 
  FOREIGN KEY ("user_id") REFERENCES "users"("id") ON DELETE CASCADE ON UPDATE CASCADE;
```

**Migration 002: Add Timestamps**

```prisma
model User {
  id        Int      @id @default(autoincrement())
  email     String   @unique
  name      String?
  createdAt DateTime @default(now()) @map("created_at")
  updatedAt DateTime @updatedAt @map("updated_at")
  
  profile   Profile?
  
  @@map("users")
}

model Profile {
  id        Int      @id @default(autoincrement())
  bio       String?
  avatar    String?
  userId    Int      @unique @map("user_id")
  createdAt DateTime @default(now()) @map("created_at")
  updatedAt DateTime @updatedAt @map("updated_at")
  
  user      User     @relation(fields: [userId], references: [id], onDelete: Cascade)
  
  @@map("profiles")
}
```

```sql
-- AlterTable
ALTER TABLE "users" ADD COLUMN "created_at" TIMESTAMP(3) NOT NULL DEFAULT CURRENT_TIMESTAMP,
ADD COLUMN "updated_at" TIMESTAMP(3) NOT NULL DEFAULT CURRENT_TIMESTAMP;

-- AlterTable
ALTER TABLE "profiles" ADD COLUMN "created_at" TIMESTAMP(3) NOT NULL DEFAULT CURRENT_TIMESTAMP,
ADD COLUMN "updated_at" TIMESTAMP(3) NOT NULL DEFAULT CURRENT_TIMESTAMP;

-- CreateTrigger (PostgreSQL specific)
CREATE OR REPLACE FUNCTION update_updated_at_column()
RETURNS TRIGGER AS $$
BEGIN
    NEW.updated_at = CURRENT_TIMESTAMP;
    RETURN NEW;
END;
$$ language 'plpgsql';

CREATE TRIGGER update_users_updated_at 
  BEFORE UPDATE ON users 
  FOR EACH ROW 
  EXECUTE FUNCTION update_updated_at_column();

CREATE TRIGGER update_profiles_updated_at 
  BEFORE UPDATE ON profiles 
  FOR EACH ROW 
  EXECUTE FUNCTION update_updated_at_column();
```

### 2. Reversible Migrations (Up/Down Approach)

Some ORMs support reversible migrations where you can roll back changes. While Prisma doesn't have built-in down migrations, you can implement rollback strategies manually.

#### **Manual Rollback Strategy**

```typescript
// migration-utils.ts
export interface MigrationStep {
  id: string
  description: string
  up: () => Promise<void>
  down: () => Promise<void>
  validate?: () => Promise<boolean>
}

export class MigrationManager {
  constructor(private prisma: PrismaClient) {}
  
  async executeMigration(step: MigrationStep): Promise<void> {
    try {
      console.log(`Executing migration: ${step.description}`)
      
      // Validate preconditions
      if (step.validate && !(await step.validate())) {
        throw new Error(`Migration validation failed: ${step.id}`)
      }
      
      // Execute migration
      await step.up()
      
      // Record migration in custom tracking table
      await this.recordMigration(step.id, step.description)
      
      console.log(`Migration completed: ${step.id}`)
    } catch (error) {
      console.error(`Migration failed: ${step.id}`, error)
      
      // Attempt rollback
      try {
        await step.down()
        console.log(`Rollback successful: ${step.id}`)
      } catch (rollbackError) {
        console.error(`Rollback failed: ${step.id}`, rollbackError)
        throw new Error(`Migration and rollback both failed: ${step.id}`)
      }
      
      throw error
    }
  }
  
  async rollbackMigration(step: MigrationStep): Promise<void> {
    console.log(`Rolling back migration: ${step.description}`)
    await step.down()
    await this.removeMigrationRecord(step.id)
    console.log(`Rollback completed: ${step.id}`)
  }
  
  private async recordMigration(id: string, description: string): Promise<void> {
    await this.prisma.$executeRaw`
      INSERT INTO custom_migrations (id, description, applied_at)
      VALUES (${id}, ${description}, NOW())
    `
  }
  
  private async removeMigrationRecord(id: string): Promise<void> {
    await this.prisma.$executeRaw`
      DELETE FROM custom_migrations WHERE id = ${id}
    `
  }
}
```

**Example: Adding Email Verification System**

```typescript
// migrations/add-email-verification.ts
export const addEmailVerificationMigration: MigrationStep = {
  id: 'add-email-verification-20231101',
  description: 'Add email verification system',
  
  async up() {
    // Add verification columns
    await prisma.$executeRaw`
      ALTER TABLE users 
      ADD COLUMN email_verified BOOLEAN DEFAULT FALSE,
      ADD COLUMN verification_token VARCHAR(255),
      ADD COLUMN verification_expires TIMESTAMP
    `
    
    // Create verification tokens table
    await prisma.$executeRaw`
      CREATE TABLE email_verification_tokens (
        id SERIAL PRIMARY KEY,
        user_id INTEGER NOT NULL REFERENCES users(id) ON DELETE CASCADE,
        token VARCHAR(255) NOT NULL UNIQUE,
        expires_at TIMESTAMP NOT NULL,
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
      )
    `
    
    // Set existing users as verified (business decision)
    await prisma.$executeRaw`
      UPDATE users SET email_verified = TRUE WHERE created_at < NOW()
    `
  },
  
  async down() {
    // Remove verification tokens table
    await prisma.$executeRaw`DROP TABLE IF EXISTS email_verification_tokens`
    
    // Remove verification columns
    await prisma.$executeRaw`
      ALTER TABLE users 
      DROP COLUMN IF EXISTS email_verified,
      DROP COLUMN IF EXISTS verification_token,
      DROP COLUMN IF EXISTS verification_expires
    `
  },
  
  async validate() {
    // Check if users table exists and has required structure
    const result = await prisma.$queryRaw`
      SELECT column_name 
      FROM information_schema.columns 
      WHERE table_name = 'users' AND column_name = 'email'
    `
    return Array.isArray(result) && result.length > 0
  }
}
```

### 3. Zero-Downtime Migrations (Blue-Green Strategy)

Zero-downtime migrations ensure your application remains available during schema changes, crucial for production systems.

#### **Expand-Contract Pattern**

The expand-contract pattern is a three-phase approach:
1. **Expand**: Add new structures alongside existing ones
2. **Migrate**: Gradually move data and update application code
3. **Contract**: Remove old structures once migration is complete

**Phase 1: Expand - Add New Structure**

```prisma
// Current schema
model User {
  id    Int    @id @default(autoincrement())
  name  String  // Will be split into firstName/lastName
  email String @unique
  
  @@map("users")
}
```

```prisma
// Expanded schema - old and new columns coexist
model User {
  id        Int     @id @default(autoincrement())
  name      String? // Keep old column during transition
  firstName String? @map("first_name") // Add new columns
  lastName  String? @map("last_name")  // Add new columns
  email     String  @unique
  
  @@map("users")
}
```

```sql
-- Migration: Expand phase
ALTER TABLE users 
ADD COLUMN first_name VARCHAR(255),
ADD COLUMN last_name VARCHAR(255);
```

**Phase 2: Migrate - Dual-Write and Data Migration**

```typescript
// Update application to write to both old and new columns
export class UserService {
  async createUser(userData: CreateUserData): Promise<User> {
    const [firstName, lastName] = this.splitName(userData.name)
    
    return await this.prisma.user.create({
      data: {
        // Write to both old and new format
        name: userData.name,           // Old format
        firstName: firstName,          // New format
        lastName: lastName,           // New format
        email: userData.email
      }
    })
  }
  
  async updateUser(id: number, userData: UpdateUserData): Promise<User> {
    const updateData: any = { ...userData }
    
    // If name is being updated, update both formats
    if (userData.name) {
      const [firstName, lastName] = this.splitName(userData.name)
      updateData.firstName = firstName
      updateData.lastName = lastName
    }
    
    // If firstName/lastName provided, update name too
    if (userData.firstName || userData.lastName) {
      const user = await this.prisma.user.findUnique({ where: { id } })
      updateData.name = `${userData.firstName || user?.firstName || ''} ${userData.lastName || user?.lastName || ''}`.trim()
    }
    
    return await this.prisma.user.update({
      where: { id },
      data: updateData
    })
  }
  
  private splitName(fullName: string): [string, string] {
    const parts = fullName.trim().split(' ')
    const firstName = parts[0] || ''
    const lastName = parts.slice(1).join(' ') || ''
    return [firstName, lastName]
  }
}
```

```typescript
// Background data migration script
async function migrateNameData() {
  const batchSize = 1000
  let offset = 0
  let processedCount = 0
  
  while (true) {
    const users = await prisma.user.findMany({
      where: {
        OR: [
          { firstName: null },
          { lastName: null }
        ]
      },
      take: batchSize,
      skip: offset
    })
    
    if (users.length === 0) break
    
    for (const user of users) {
      if (user.name && (!user.firstName || !user.lastName)) {
        const [firstName, lastName] = splitName(user.name)
        
        await prisma.user.update({
          where: { id: user.id },
          data: {
            firstName,
            lastName
          }
        })
        
        processedCount++
      }
    }
    
    offset += batchSize
    console.log(`Migrated ${processedCount} users...`)
    
    // Add small delay to avoid overwhelming the database
    await new Promise(resolve => setTimeout(resolve, 100))
  }
  
  console.log(`Migration complete. Processed ${processedCount} users.`)
}
```

**Phase 3: Contract - Remove Old Structure**

```typescript
// Update application to use only new columns
export class UserService {
  async createUser(userData: CreateUserData): Promise<User> {
    return await this.prisma.user.create({
      data: {
        firstName: userData.firstName,
        lastName: userData.lastName,
        email: userData.email
      }
    })
  }
  
  async getFullName(user: User): string {
    return `${user.firstName || ''} ${user.lastName || ''}`.trim()
  }
}
```

```prisma
// Final schema - old column removed
model User {
  id        Int    @id @default(autoincrement())
  firstName String @map("first_name")
  lastName  String @map("last_name")
  email     String @unique
  
  @@map("users")
}
```

```sql
-- Migration: Contract phase
ALTER TABLE users DROP COLUMN name;
```

### 4. Schema Versioning Strategies

#### **API Versioning with Schema Evolution**

```typescript
// version-aware-service.ts
export class VersionAwareUserService {
  constructor(private prisma: PrismaClient) {}
  
  async getUser(id: number, apiVersion: string = 'v2'): Promise<UserResponse> {
    const user = await this.prisma.user.findUnique({
      where: { id },
      include: { profile: true }
    })
    
    if (!user) throw new Error('User not found')
    
    // Transform based on API version
    switch (apiVersion) {
      case 'v1':
        return this.transformToV1(user)
      case 'v2':
        return this.transformToV2(user)
      default:
        return this.transformToV2(user) // Default to latest
    }
  }
  
  private transformToV1(user: any): UserV1Response {
    return {
      id: user.id,
      name: `${user.firstName} ${user.lastName}`.trim(), // Combine for v1
      email: user.email,
      bio: user.profile?.bio || null
    }
  }
  
  private transformToV2(user: any): UserV2Response {
    return {
      id: user.id,
      firstName: user.firstName,
      lastName: user.lastName,
      email: user.email,
      profile: user.profile ? {
        bio: user.profile.bio,
        avatar: user.profile.avatar,
        createdAt: user.profile.createdAt
      } : null
    }
  }
}
```

#### **Database Views for Backward Compatibility**

```sql
-- Create backward-compatible view for v1 API
CREATE OR REPLACE VIEW users_v1 AS
SELECT 
  id,
  CONCAT(first_name, ' ', last_name) as name,
  email,
  created_at,
  updated_at
FROM users;

-- Create view for complex data transformations
CREATE OR REPLACE VIEW user_summary AS
SELECT 
  u.id,
  u.first_name,
  u.last_name,
  u.email,
  p.bio,
  p.avatar,
  COUNT(posts.id) as post_count,
  u.created_at as user_since
FROM users u
LEFT JOIN profiles p ON u.id = p.user_id
LEFT JOIN posts ON u.id = posts.author_id
GROUP BY u.id, u.first_name, u.last_name, u.email, p.bio, p.avatar, u.created_at;
```

---

## ðŸ”„ Complex Migration Scenarios

### Scenario 1: Splitting a Table

**Business Requirement**: Split user data into separate users and profiles tables for better organization.

```typescript
// complex-migration-split-table.ts
export const splitUserTableMigration: MigrationStep = {
  id: 'split-user-table-20231101',
  description: 'Split user table into users and profiles',
  
  async up() {
    // Step 1: Create new profiles table
    await prisma.$executeRaw`
      CREATE TABLE profiles (
        id SERIAL PRIMARY KEY,
        user_id INTEGER NOT NULL UNIQUE,
        bio TEXT,
        avatar VARCHAR(255),
        website VARCHAR(255),
        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
        FOREIGN KEY (user_id) REFERENCES users(id) ON DELETE CASCADE
      )
    `
    
    // Step 2: Add trigger for updated_at
    await prisma.$executeRaw`
      CREATE TRIGGER update_profiles_updated_at 
        BEFORE UPDATE ON profiles 
        FOR EACH ROW 
        EXECUTE FUNCTION update_updated_at_column()
    `
    
    // Step 3: Migrate existing data
    await prisma.$executeRaw`
      INSERT INTO profiles (user_id, bio, avatar, website, created_at)
      SELECT 
        id, 
        bio, 
        avatar, 
        website, 
        created_at
      FROM users 
      WHERE bio IS NOT NULL OR avatar IS NOT NULL OR website IS NOT NULL
    `
    
    // Step 4: Create profiles for users without profile data
    await prisma.$executeRaw`
      INSERT INTO profiles (user_id, created_at)
      SELECT id, created_at
      FROM users u
      WHERE NOT EXISTS (SELECT 1 FROM profiles p WHERE p.user_id = u.id)
    `
    
    // Step 5: Remove profile columns from users (contract phase)
    await prisma.$executeRaw`
      ALTER TABLE users 
      DROP COLUMN IF EXISTS bio,
      DROP COLUMN IF EXISTS avatar,
      DROP COLUMN IF EXISTS website
    `
  },
  
  async down() {
    // Step 1: Add profile columns back to users
    await prisma.$executeRaw`
      ALTER TABLE users 
      ADD COLUMN bio TEXT,
      ADD COLUMN avatar VARCHAR(255),
      ADD COLUMN website VARCHAR(255)
    `
    
    // Step 2: Migrate data back from profiles
    await prisma.$executeRaw`
      UPDATE users 
      SET 
        bio = p.bio,
        avatar = p.avatar,
        website = p.website
      FROM profiles p
      WHERE users.id = p.user_id
    `
    
    // Step 3: Drop profiles table
    await prisma.$executeRaw`DROP TABLE profiles`
  },
  
  async validate() {
    // Ensure users table exists
    const userTable = await prisma.$queryRaw`
      SELECT table_name 
      FROM information_schema.tables 
      WHERE table_name = 'users'
    `
    return Array.isArray(userTable) && userTable.length > 0
  }
}
```

### Scenario 2: Changing Data Types

**Business Requirement**: Change user IDs from integer to UUID for better scalability.

```typescript
// migration-change-id-type.ts
export const changeUserIdToUuidMigration: MigrationStep = {
  id: 'change-user-id-to-uuid-20231101',
  description: 'Change user ID from integer to UUID',
  
  async up() {
    // Step 1: Add new UUID column
    await prisma.$executeRaw`
      ALTER TABLE users ADD COLUMN uuid_id UUID DEFAULT gen_random_uuid()
    `
    
    // Step 2: Generate UUIDs for existing users
    await prisma.$executeRaw`
      UPDATE users SET uuid_id = gen_random_uuid() WHERE uuid_id IS NULL
    `
    
    // Step 3: Add UUID columns to related tables
    await prisma.$executeRaw`
      ALTER TABLE profiles ADD COLUMN user_uuid_id UUID
    `
    
    await prisma.$executeRaw`
      ALTER TABLE posts ADD COLUMN author_uuid_id UUID
    `
    
    // Step 4: Populate UUID foreign keys
    await prisma.$executeRaw`
      UPDATE profiles 
      SET user_uuid_id = u.uuid_id
      FROM users u
      WHERE profiles.user_id = u.id
    `
    
    await prisma.$executeRaw`
      UPDATE posts 
      SET author_uuid_id = u.uuid_id
      FROM users u
      WHERE posts.author_id = u.id
    `
    
    // Step 5: Create unique constraints on new UUID columns
    await prisma.$executeRaw`
      ALTER TABLE users ADD CONSTRAINT users_uuid_id_unique UNIQUE (uuid_id)
    `
    
    // Step 6: Drop old foreign key constraints
    await prisma.$executeRaw`
      ALTER TABLE profiles DROP CONSTRAINT profiles_user_id_fkey
    `
    
    await prisma.$executeRaw`
      ALTER TABLE posts DROP CONSTRAINT posts_author_id_fkey
    `
    
    // Step 7: Add new foreign key constraints
    await prisma.$executeRaw`
      ALTER TABLE profiles 
      ADD CONSTRAINT profiles_user_uuid_id_fkey 
      FOREIGN KEY (user_uuid_id) REFERENCES users(uuid_id) ON DELETE CASCADE
    `
    
    await prisma.$executeRaw`
      ALTER TABLE posts 
      ADD CONSTRAINT posts_author_uuid_id_fkey 
      FOREIGN KEY (author_uuid_id) REFERENCES users(uuid_id) ON DELETE CASCADE
    `
    
    // Step 8: Update primary key (requires careful handling)
    await prisma.$executeRaw`
      ALTER TABLE users DROP CONSTRAINT users_pkey
    `
    
    await prisma.$executeRaw`
      ALTER TABLE users ADD CONSTRAINT users_pkey PRIMARY KEY (uuid_id)
    `
    
    // Step 9: Drop old integer columns
    await prisma.$executeRaw`
      ALTER TABLE profiles DROP COLUMN user_id
    `
    
    await prisma.$executeRaw`
      ALTER TABLE posts DROP COLUMN author_id
    `
    
    await prisma.$executeRaw`
      ALTER TABLE users DROP COLUMN id
    `
    
    // Step 10: Rename UUID columns to final names
    await prisma.$executeRaw`
      ALTER TABLE users RENAME COLUMN uuid_id TO id
    `
    
    await prisma.$executeRaw`
      ALTER TABLE profiles RENAME COLUMN user_uuid_id TO user_id
    `
    
    await prisma.$executeRaw`
      ALTER TABLE posts RENAME COLUMN author_uuid_id TO author_id
    `
  },
  
  async down() {
    // This is a complex rollback that would require careful planning
    // In practice, this type of migration might be irreversible
    throw new Error('UUID to integer rollback not implemented - requires manual intervention')
  },
  
  async validate() {
    // Check if UUID extension is available
    const uuidExtension = await prisma.$queryRaw`
      SELECT * FROM pg_extension WHERE extname = 'pgcrypto'
    `
    
    if (!Array.isArray(uuidExtension) || uuidExtension.length === 0) {
      // Enable UUID extension if not available
      await prisma.$executeRaw`CREATE EXTENSION IF NOT EXISTS "pgcrypto"`
    }
    
    return true
  }
}
```

---

## ðŸš€ Production Migration Best Practices

### 1. Pre-Migration Checklist

```typescript
// pre-migration-check.ts
export class PreMigrationValidator {
  constructor(private prisma: PrismaClient) {}
  
  async validateMigrationSafety(migration: MigrationStep): Promise<ValidationResult> {
    const checks: ValidationCheck[] = [
      await this.checkDatabaseConnection(),
      await this.checkDiskSpace(),
      await this.checkActiveConnections(),
      await this.checkTableLocks(),
      await this.validateBackup(),
      await this.checkDependencies(),
      await this.validateRollbackPlan()
    ]
    
    const failures = checks.filter(check => !check.passed)
    
    return {
      passed: failures.length === 0,
      checks,
      failures,
      canProceed: failures.every(f => f.severity !== 'critical')
    }
  }
  
  private async checkDatabaseConnection(): Promise<ValidationCheck> {
    try {
      await this.prisma.$queryRaw`SELECT 1`
      return {
        name: 'Database Connection',
        passed: true,
        message: 'Database connection is healthy'
      }
    } catch (error) {
      return {
        name: 'Database Connection',
        passed: false,
        severity: 'critical',
        message: `Database connection failed: ${error.message}`
      }
    }
  }
  
  private async checkDiskSpace(): Promise<ValidationCheck> {
    const result = await this.prisma.$queryRaw`
      SELECT 
        pg_size_pretty(pg_database_size(current_database())) as db_size,
        pg_size_pretty(pg_total_relation_size('users')) as largest_table_size
    ` as any[]
    
    // In a real implementation, you'd check available disk space
    // and compare with estimated migration space requirements
    return {
      name: 'Disk Space',
      passed: true,
      message: `Database size: ${result[0]?.db_size || 'unknown'}`
    }
  }
  
  private async checkActiveConnections(): Promise<ValidationCheck> {
    const connections = await this.prisma.$queryRaw`
      SELECT count(*) as active_connections
      FROM pg_stat_activity 
      WHERE state = 'active'
    ` as any[]
    
    const activeCount = parseInt(connections[0]?.active_connections || '0')
    const isHealthy = activeCount < 50 // Threshold for your system
    
    return {
      name: 'Active Connections',
      passed: isHealthy,
      severity: isHealthy ? 'info' : 'warning',
      message: `${activeCount} active connections`
    }
  }
  
  private async checkTableLocks(): Promise<ValidationCheck> {
    const locks = await this.prisma.$queryRaw`
      SELECT 
        schemaname,
        tablename,
        mode
      FROM pg_locks l
      JOIN pg_class c ON l.relation = c.oid
      JOIN pg_namespace n ON c.relnamespace = n.oid
      WHERE l.mode LIKE '%ExclusiveLock%'
    ` as any[]
    
    return {
      name: 'Table Locks',
      passed: locks.length === 0,
      severity: locks.length > 0 ? 'warning' : 'info',
      message: locks.length > 0 ? 
        `${locks.length} exclusive locks detected` : 
        'No blocking locks detected'
    }
  }
  
  private async validateBackup(): Promise<ValidationCheck> {
    // Check if recent backup exists
    // This would integrate with your backup system
    const backupAge = await this.getLastBackupAge()
    const isRecent = backupAge < 24 // Less than 24 hours
    
    return {
      name: 'Recent Backup',
      passed: isRecent,
      severity: isRecent ? 'info' : 'critical',
      message: isRecent ? 
        `Backup is ${backupAge} hours old` : 
        `Last backup is ${backupAge} hours old - create fresh backup`
    }
  }
  
  private async getLastBackupAge(): Promise<number> {
    // Placeholder - integrate with your backup system
    return 2 // Assume 2 hours for demo
  }
  
  private async checkDependencies(): Promise<ValidationCheck> {
    // Check if required database extensions are available
    const extensions = await this.prisma.$queryRaw`
      SELECT extname FROM pg_extension 
      WHERE extname IN ('pgcrypto', 'uuid-ossp')
    ` as any[]
    
    return {
      name: 'Dependencies',
      passed: true,
      message: `Extensions available: ${extensions.map(e => e.extname).join(', ')}`
    }
  }
  
  private async validateRollbackPlan(): Promise<ValidationCheck> {
    // Verify rollback procedures are documented and tested
    return {
      name: 'Rollback Plan',
      passed: true,
      message: 'Rollback procedures documented'
    }
  }
}

interface ValidationResult {
  passed: boolean
  checks: ValidationCheck[]
  failures: ValidationCheck[]
  canProceed: boolean
}

interface ValidationCheck {
  name: string
  passed: boolean
  severity?: 'info' | 'warning' | 'critical'
  message: string
}
```

### 2. Migration Execution Framework

```typescript
// migration-executor.ts
export class ProductionMigrationExecutor {
  constructor(
    private prisma: PrismaClient,
    private logger: Logger,
    private notificationService: NotificationService
  ) {}
  
  async executeMigration(migration: MigrationStep): Promise<MigrationResult> {
    const startTime = Date.now()
    const migrationId = `${migration.id}-${startTime}`
    
    try {
      // Pre-migration validation
      await this.validatePreconditions(migration)
      
      // Create backup point
      const backupId = await this.createBackupPoint()
      
      // Begin transaction for rollback capability
      const result = await this.prisma.$transaction(async (tx) => {
        // Execute migration in transaction
        await this.executeMigrationSteps(migration, tx)
        
        // Validate post-migration state
        await this.validatePostMigration(migration, tx)
        
        return { success: true, backupId }
      }, {
        timeout: 300000, // 5 minute timeout
        isolationLevel: 'Serializable'
      })
      
      // Log success
      const duration = Date.now() - startTime
      await this.logMigrationSuccess(migrationId, duration)
      
      // Notify team
      await this.notificationService.notifySuccess(migration, duration)
      
      return {
        success: true,
        migrationId,
        duration,
        backupId: result.backupId
      }
      
    } catch (error) {
      // Log failure
      await this.logMigrationFailure(migrationId, error)
      
      // Attempt automatic rollback
      await this.attemptRollback(migration, migrationId)
      
      // Notify team of failure
      await this.notificationService.notifyFailure(migration, error)
      
      throw new MigrationError(
        `Migration failed: ${migration.id}`,
        error,
        migrationId
      )
    }
  }
  
  private async validatePreconditions(migration: MigrationStep): Promise<void> {
    const validator = new PreMigrationValidator(this.prisma)
    const validation = await validator.validateMigrationSafety(migration)
    
    if (!validation.canProceed) {
      throw new Error(
        `Migration preconditions failed: ${
          validation.failures.map(f => f.message).join(', ')
        }`
      )
    }
    
    if (validation.failures.length > 0) {
      this.logger.warn('Migration proceeding with warnings:', validation.failures)
    }
  }
  
  private async createBackupPoint(): Promise<string> {
    const backupId = `migration-backup-${Date.now()}`
    
    // In production, this would create a database backup
    // For example, using pg_dump for PostgreSQL
    this.logger.info(`Creating backup point: ${backupId}`)
    
    // Placeholder for actual backup creation
    return backupId
  }
  
  private async executeMigrationSteps(
    migration: MigrationStep, 
    tx: any
  ): Promise<void> {
    this.logger.info(`Executing migration: ${migration.description}`)
    
    // Execute the migration with the transaction context
    await migration.up()
    
    this.logger.info(`Migration steps completed: ${migration.id}`)
  }
  
  private async validatePostMigration(
    migration: MigrationStep,
    tx: any
  ): Promise<void> {
    if (migration.validate) {
      const isValid = await migration.validate()
      if (!isValid) {
        throw new Error(`Post-migration validation failed: ${migration.id}`)
      }
    }
    
    // Additional post-migration checks
    await this.validateDataIntegrity(tx)
    await this.validateConstraints(tx)
  }
  
  private async validateDataIntegrity(tx: any): Promise<void> {
    // Check for orphaned records, invalid references, etc.
    const orphanedProfiles = await tx.$queryRaw`
      SELECT COUNT(*) as count
      FROM profiles p
      LEFT JOIN users u ON p.user_id = u.id
      WHERE u.id IS NULL
    `
    
    if (orphanedProfiles[0]?.count > 0) {
      throw new Error(`Found ${orphanedProfiles[0].count} orphaned profile records`)
    }
  }
  
  private async validateConstraints(tx: any): Promise<void> {
    // Verify all constraints are properly enforced
    const constraintViolations = await tx.$queryRaw`
      SELECT 
        conname as constraint_name,
        conrelid::regclass as table_name
      FROM pg_constraint
      WHERE NOT pg_constraint_is_visible(oid)
    `
    
    if (constraintViolations.length > 0) {
      throw new Error(`Constraint violations detected: ${constraintViolations.map(c => c.constraint_name).join(', ')}`)
    }
  }
  
  private async attemptRollback(
    migration: MigrationStep,
    migrationId: string
  ): Promise<void> {
    try {
      this.logger.warn(`Attempting rollback for migration: ${migrationId}`)
      await migration.down()
      this.logger.info(`Rollback successful: ${migrationId}`)
    } catch (rollbackError) {
      this.logger.error(`Rollback failed: ${migrationId}`, rollbackError)
      // In this case, manual intervention is required
      await this.notificationService.notifyRollbackFailure(migration, rollbackError)
    }
  }
  
  private async logMigrationSuccess(
    migrationId: string,
    duration: number
  ): Promise<void> {
    this.logger.info(`Migration completed successfully: ${migrationId} (${duration}ms)`)
    
    // Store in migration history table
    await this.prisma.$executeRaw`
      INSERT INTO migration_history (
        migration_id, 
        status, 
        duration_ms, 
        executed_at
      ) VALUES (
        ${migrationId}, 
        'success', 
        ${duration}, 
        NOW()
      )
    `
  }
  
  private async logMigrationFailure(
    migrationId: string,
    error: any
  ): Promise<void> {
    this.logger.error(`Migration failed: ${migrationId}`, error)
    
    await this.prisma.$executeRaw`
      INSERT INTO migration_history (
        migration_id, 
        status, 
        error_message, 
        executed_at
      ) VALUES (
        ${migrationId}, 
        'failed', 
        ${error.message}, 
        NOW()
      )
    `
  }
}

interface MigrationResult {
  success: boolean
  migrationId: string
  duration: number
  backupId: string
}

class MigrationError extends Error {
  constructor(
    message: string,
    public cause: any,
    public migrationId: string
  ) {
    super(message)
  }
}
```

### 3. Monitoring and Alerting

```typescript
// migration-monitoring.ts
export class MigrationMonitor {
  constructor(
    private prisma: PrismaClient,
    private metricsCollector: MetricsCollector,
    private alertManager: AlertManager
  ) {}
  
  async monitorMigrationProgress(migrationId: string): Promise<void> {
    const startTime = Date.now()
    
    const monitoringInterval = setInterval(async () => {
      try {
        const metrics = await this.collectMigrationMetrics()
        await this.metricsCollector.record('migration_progress', metrics)
        
        // Check for performance degradation
        if (metrics.activeConnections > 100) {
          await this.alertManager.warn('High connection count during migration', {
            migrationId,
            activeConnections: metrics.activeConnections
          })
        }
        
        if (metrics.lockWaitTime > 5000) {
          await this.alertManager.warn('High lock wait times during migration', {
            migrationId,
            lockWaitTime: metrics.lockWaitTime
          })
        }
        
      } catch (error) {
        await this.alertManager.error('Migration monitoring failed', {
          migrationId,
          error: error.message
        })
      }
    }, 10000) // Check every 10 seconds
    
    // Clean up monitoring when migration completes
    setTimeout(() => {
      clearInterval(monitoringInterval)
    }, 600000) // 10 minute max monitoring
  }
  
  private async collectMigrationMetrics(): Promise<MigrationMetrics> {
    const [
      connectionStats,
      lockStats,
      performanceStats
    ] = await Promise.all([
      this.getConnectionStats(),
      this.getLockStats(),
      this.getPerformanceStats()
    ])
    
    return {
      timestamp: new Date(),
      activeConnections: connectionStats.active,
      idleConnections: connectionStats.idle,
      lockWaitTime: lockStats.maxWaitTime,
      blockedQueries: lockStats.blockedCount,
      avgQueryTime: performanceStats.avgDuration,
      slowQueries: performanceStats.slowQueries
    }
  }
  
  private async getConnectionStats(): Promise<ConnectionStats> {
    const stats = await this.prisma.$queryRaw`
      SELECT 
        COUNT(CASE WHEN state = 'active' THEN 1 END) as active,
        COUNT(CASE WHEN state = 'idle' THEN 1 END) as idle
      FROM pg_stat_activity
      WHERE datname = current_database()
    ` as any[]
    
    return {
      active: parseInt(stats[0]?.active || '0'),
      idle: parseInt(stats[0]?.idle || '0')
    }
  }
  
  private async getLockStats(): Promise<LockStats> {
    const stats = await this.prisma.$queryRaw`
      SELECT 
        COUNT(*) as blocked_count,
        COALESCE(MAX(EXTRACT(EPOCH FROM (now() - query_start)) * 1000), 0) as max_wait_time
      FROM pg_stat_activity
      WHERE wait_event_type = 'Lock'
    ` as any[]
    
    return {
      blockedCount: parseInt(stats[0]?.blocked_count || '0'),
      maxWaitTime: parseFloat(stats[0]?.max_wait_time || '0')
    }
  }
  
  private async getPerformanceStats(): Promise<PerformanceStats> {
    const stats = await this.prisma.$queryRaw`
      SELECT 
        AVG(mean_exec_time) as avg_duration,
        COUNT(CASE WHEN mean_exec_time > 1000 THEN 1 END) as slow_queries
      FROM pg_stat_statements
      WHERE last_exec > now() - interval '1 minute'
    ` as any[]
    
    return {
      avgDuration: parseFloat(stats[0]?.avg_duration || '0'),
      slowQueries: parseInt(stats[0]?.slow_queries || '0')
    }
  }
}

interface MigrationMetrics {
  timestamp: Date
  activeConnections: number
  idleConnections: number
  lockWaitTime: number
  blockedQueries: number
  avgQueryTime: number
  slowQueries: number
}

interface ConnectionStats {
  active: number
  idle: number
}

interface LockStats {
  blockedCount: number
  maxWaitTime: number
}

interface PerformanceStats {
  avgDuration: number
  slowQueries: number
}
```

---

## ðŸ“Š Migration Strategies Comparison

| Strategy | Downtime | Complexity | Risk Level | Best For |
|----------|----------|------------|------------|----------|
| **Forward-Only** | âš¡ Low | ðŸ“ˆ Low | ðŸŸ¢ Low | Development, simple changes |
| **Reversible** | âš¡ Low | ðŸ“ˆ Medium | ðŸŸ¡ Medium | Feature rollbacks, testing |
| **Blue-Green** | âš¡ None | ðŸ“ˆ High | ðŸŸ¢ Low | Production, critical systems |
| **Expand-Contract** | âš¡ None | ðŸ“ˆ High | ðŸŸ¡ Medium | Breaking changes, major refactors |

---

## ðŸ§  Knowledge Check

### Migration Strategy Quiz

1. **Which migration strategy is best for zero-downtime deployments?**
   - [ ] A) Forward-Only
   - [ ] B) Reversible
   - [x] C) Expand-Contract
   - [ ] D) Schema Versioning

   **Explanation**: Expand-Contract allows you to maintain compatibility during the transition, achieving zero downtime.

2. **What is the primary risk of complex migrations?**
   - [ ] A) Slower performance
   - [x] B) Data loss or corruption
   - [ ] C) Increased storage costs
   - [ ] D) API breaking changes

   **Explanation**: Data integrity is the primary concern, as migration failures can lead to data loss or corruption.

3. **In the Expand-Contract pattern, what happens during the "Contract" phase?**
   - [ ] A) Add new schema structures
   - [ ] B) Migrate data between old and new structures
   - [x] C) Remove old, deprecated structures
   - [ ] D) Validate data integrity

   **Explanation**: The Contract phase removes the old structures after migration is complete and new code is deployed.

### Practical Exercise: Migration Planning

**Scenario**: Design a migration strategy for the following requirement:

**Business Requirement**: 
- Change user email from single field to support multiple emails
- Maintain backward compatibility for 6 months
- Zero downtime deployment required
- 10M+ existing users

**Your Migration Plan**:

```typescript
// Phase 1: Expand (Design your expansion strategy)
// - What new tables/columns will you add?
// - How will you maintain data consistency?

// Phase 2: Migrate (Design your data migration)
// - How will you handle dual-write scenarios?
// - What's your strategy for migrating 10M records?

// Phase 3: Contract (Design your cleanup strategy)
// - When is it safe to remove old structures?
// - How will you verify migration success?

// Rollback Plan (Design your contingency)
// - What are the rollback trigger points?
// - How will you handle partial failures?
```

### Migration Safety Checklist

**Before Every Production Migration**:

- [ ] âœ… Fresh database backup created (< 1 hour old)
- [ ] âœ… Migration tested in staging environment
- [ ] âœ… Rollback procedure documented and tested
- [ ] âœ… Monitoring and alerting configured
- [ ] âœ… Team notified of maintenance window
- [ ] âœ… Database performance baseline established
- [ ] âœ… Migration execution time estimated
- [ ] âœ… Post-migration validation plan ready

---

## ðŸ’¡ Key Takeaways

- ðŸ›« **Strategic Planning**: Like airport renovations, database migrations require careful planning and phased execution
- ðŸ”„ **Multiple Strategies**: Different migration strategies serve different needs - choose based on requirements
- ðŸ“Š **Zero Downtime**: Expand-Contract pattern enables zero-downtime migrations for production systems
- ðŸ›¡ï¸ **Safety First**: Always have backup and rollback strategies before executing migrations
- ðŸ“ˆ **Monitoring**: Real-time monitoring during migrations helps detect issues early
- ðŸ‘¥ **Team Coordination**: Complex migrations require clear communication and coordination
- ðŸ”§ **Tooling**: Invest in migration tooling and automation for consistent, reliable deployments
- ðŸ“š **Documentation**: Document migration procedures and lessons learned for future reference

---

## ðŸ”— Navigation

**ðŸ“ Current Location**: Module 1 â†’ Section 1.3B â†’ Lesson 1.3B.3

**â¬…ï¸ Previous**: [1.4.2 Code-First vs Database-First Approaches](./1.4.2-code-first-vs-database-first-approaches.md)
**âž¡ï¸ Next**: [1.4.4 Active Record vs Data Mapper Patterns](./1.4.4-active-record-vs-data-mapper-patterns.md)

**ðŸ  Section Home**: [1.3B ORM Concepts](./README.md)
**ðŸ“š Module Home**: [Module 1: Foundations](../01-foundations.md)

**ðŸ—ºï¸ Quick Links**:
- [Previous: Development Approaches](./1.3b.2-code-first-vs-database-first-approaches.md)
- [Next: Design Patterns](./1.3b.4-active-record-vs-data-mapper-patterns.md)
- [Jump to: Development Environment](../1.4-development-environment/)

---

*Master migration strategies are essential for maintaining production systems. Ready to explore ORM design patterns? Continue to the next lesson!*
